{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f5d069cd26a416c9b4f59a32dac40b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c8524b78dc4ca2807521fe51161169",
              "IPY_MODEL_cd7c9a5fdb6e445ebc913515e3c68c9a",
              "IPY_MODEL_d45664a1b89d417096675db8ad71b28a"
            ],
            "layout": "IPY_MODEL_63a9395bc99b4b6fa26c7738cf1b6d33"
          }
        },
        "97c8524b78dc4ca2807521fe51161169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e547ce065046c99e3e400a2e206819",
            "placeholder": "​",
            "style": "IPY_MODEL_e4792eaa56b14da4a3611af50c92f2c1",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "cd7c9a5fdb6e445ebc913515e3c68c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b20f26e1ddf4ccb940e291fce70ed47",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90e5922019aa4be7bfdeaa1c10040ec0",
            "value": 2
          }
        },
        "d45664a1b89d417096675db8ad71b28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9619226a55bc4a2194e534d9a8cc1087",
            "placeholder": "​",
            "style": "IPY_MODEL_3627e2c1b088498e976ebefec65b0042",
            "value": " 2/2 [00:02&lt;00:00,  0.83it/s]"
          }
        },
        "63a9395bc99b4b6fa26c7738cf1b6d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "f4e547ce065046c99e3e400a2e206819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4792eaa56b14da4a3611af50c92f2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b20f26e1ddf4ccb940e291fce70ed47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e5922019aa4be7bfdeaa1c10040ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9619226a55bc4a2194e534d9a8cc1087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3627e2c1b088498e976ebefec65b0042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84f210de653e4df7a523331b4166ebb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c56f837cfff1476984190a5a90b8b56d",
              "IPY_MODEL_fac267d82a5541e7a2fb8567ed6fc8cb",
              "IPY_MODEL_90e2ed1492e74fa888fdc89ca5f45911"
            ],
            "layout": "IPY_MODEL_88d01849542945aa91424183e8c371fb"
          }
        },
        "c56f837cfff1476984190a5a90b8b56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_022235382f0d4095ba1c2557eea5b1cb",
            "placeholder": "​",
            "style": "IPY_MODEL_34aea251c76d46149789cd73bca5733c",
            "value": "Epoch 0: 100%"
          }
        },
        "fac267d82a5541e7a2fb8567ed6fc8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8a2e0b31744d81b3e7fe9c08cf0e68",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c758b8801bf4cf18ebecf3488d3486e",
            "value": 40
          }
        },
        "90e2ed1492e74fa888fdc89ca5f45911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51f17ecd27fb45d1863a88a97c1c2b29",
            "placeholder": "​",
            "style": "IPY_MODEL_30c733a00cc4446aa02acbe5beba72e8",
            "value": " 40/40 [00:08&lt;00:00,  4.86it/s, v_num=0]"
          }
        },
        "88d01849542945aa91424183e8c371fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "022235382f0d4095ba1c2557eea5b1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34aea251c76d46149789cd73bca5733c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c8a2e0b31744d81b3e7fe9c08cf0e68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c758b8801bf4cf18ebecf3488d3486e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51f17ecd27fb45d1863a88a97c1c2b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c733a00cc4446aa02acbe5beba72e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "  !pip install librosa\n",
        "!pip install mir_eval\n",
        "!pip install numpy\n",
        "!pip install pytorch_lightning\n",
        "!pip install torch\n",
        "!pip install torchaudio"
      ],
      "metadata": {
        "id": "WVGjTSptjzN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1126644-41ce-48f5-dc2c-57c638c84362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "Collecting mir_eval\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.11.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mir_eval) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mir_eval) (1.16.0)\n",
            "Building wheels for collected packages: mir_eval\n",
            "  Building wheel for mir_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir_eval: filename=mir_eval-0.7-py3-none-any.whl size=100703 sha256=33d8fef3108154eae9bd756736141b7662d3f258b126e116aa94fae65284e101\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/2f/0d/dda9c4c77a170e21356b6afa2f7d9bb078338634ba05d94e3f\n",
            "Successfully built mir_eval\n",
            "Installing collected packages: mir_eval\n",
            "Successfully installed mir_eval-0.7\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.10.0 pytorch_lightning-2.1.2 torchmetrics-1.2.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import librosa\n",
        "import mir_eval\n",
        "import numpy as np\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "from random import shuffle\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as tud\n",
        "import torchaudio\n",
        "import scipy.signal"
      ],
      "metadata": {
        "id": "T0K3SrPDbJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE TRAIN/VAL/TEST SPLITS IN DATA ##\n",
        "\n",
        "audio_dir = \"./data/BallroomData/\"\n",
        "\n",
        "audio_files = glob(os.path.join(audio_dir, \"**\", \"*.wav\"))\n",
        "print(f\"# of Audio files: {len(audio_files)}\")\n",
        "\n",
        "g = th.Generator().manual_seed(2147483647)\n",
        "\n",
        "# test split\n",
        "test_size = int(len(audio_files) * 0.1)\n",
        "rp = th.randperm(len(audio_files), generator=g).tolist()\n",
        "temp_audio_files = [audio_files[i] for i in rp[:-test_size]]\n",
        "test_audio_files = [audio_files[i] for i in rp[-test_size:]]\n",
        "print(f\"# of Test files: {len(test_audio_files)}\")\n",
        "\n",
        "# train / val split\n",
        "val_size = int(len(temp_audio_files) * 0.11111)\n",
        "rp = th.randperm(len(temp_audio_files), generator=g).tolist()\n",
        "train_audio_files = [temp_audio_files[i] for i in rp[:-val_size]]\n",
        "print(f\"# of Train files: {len(train_audio_files)}\")\n",
        "val_audio_files = [temp_audio_files[i] for i in rp[-val_size:]]\n",
        "print(f\"# of Val files: {len(val_audio_files)}\")\n",
        "\n",
        "# train\n",
        "train_file_path = os.path.join(audio_dir, \"train.txt\")\n",
        "with open(train_file_path, \"w\") as f:\n",
        "    for file in train_audio_files:\n",
        "        f.write(f\"{os.path.relpath(file, start=audio_dir)}\\n\")\n",
        "\n",
        "# val\n",
        "val_file_path = os.path.join(audio_dir, \"val.txt\")\n",
        "with open(val_file_path, \"w\") as f:\n",
        "    for file in val_audio_files:\n",
        "        f.write(f\"{os.path.relpath(file, start=audio_dir)}\\n\")\n",
        "\n",
        "# test\n",
        "test_file_path = os.path.join(audio_dir, \"test.txt\")\n",
        "with open(test_file_path, \"w\") as f:\n",
        "    for file in test_audio_files:\n",
        "        f.write(f\"{os.path.relpath(file, start=audio_dir)}\\n\")"
      ],
      "metadata": {
        "id": "SEAN4Erybwxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##HARMONICS TFT BLOCK##\n",
        "\n",
        "def hz_to_midi(hz):\n",
        "    return 12 * (th.log2(hz) - np.log2(440.0)) + 69\n",
        "\n",
        "\n",
        "def midi_to_hz(midi):\n",
        "    return 440.0 * (2.0 ** ((midi - 69.0)/12.0))\n",
        "\n",
        "\n",
        "def note_to_midi(note):\n",
        "    return librosa.core.note_to_midi(note)\n",
        "\n",
        "\n",
        "def hz_to_note(hz):\n",
        "    return librosa.core.hz_to_note(hz)\n",
        "\n",
        "\n",
        "def initialize_filterbank(sample_rate, n_harmonic, semitone_scale):\n",
        "    # MIDI\n",
        "    # lowest note\n",
        "    low_midi = note_to_midi('C1')\n",
        "    # highest note\n",
        "    high_note = hz_to_note(sample_rate / (2 * n_harmonic))\n",
        "    high_midi = note_to_midi(high_note)\n",
        "    # number of scales\n",
        "    level = (high_midi - low_midi) * semitone_scale\n",
        "    midi = np.linspace(low_midi, high_midi, level + 1)\n",
        "    hz = midi_to_hz(midi[:-1])\n",
        "    # stack harmonics\n",
        "    harmonic_hz = []\n",
        "    for i in range(n_harmonic):\n",
        "        harmonic_hz = np.concatenate((harmonic_hz, hz * (i+1)))\n",
        "    return harmonic_hz, level\n",
        "\n",
        "\n",
        "class HarmonicSTFT(nn.Module):\n",
        "    \"\"\"\n",
        "    Trainable harmonic filters as implemented by Minz Won.\n",
        "\n",
        "    Paper: https://ccrma.stanford.edu/~urinieto/MARL/publications/ICASSP2020_Won.pdf\n",
        "    Code: https://github.com/minzwon/data-driven-harmonic-filters\n",
        "    Pretrained: https://github.com/minzwon/sota-music-tagging-models/tree/master/training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sample_rate=16000,\n",
        "                 n_fft=513,\n",
        "                 win_length=None,\n",
        "                 hop_length=None,\n",
        "                 pad=0,\n",
        "                 power=2,\n",
        "                 normalized=False,\n",
        "                 n_harmonic=6,\n",
        "                 semitone_scale=2,\n",
        "                 bw_Q=1.0,\n",
        "                 learn_bw=None,\n",
        "                 checkpoint=None):\n",
        "        super(HarmonicSTFT, self).__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_harmonic = n_harmonic\n",
        "        self.bw_alpha = 0.1079\n",
        "        self.bw_beta = 24.7\n",
        "\n",
        "        # Spectrogram\n",
        "        self.spec = torchaudio.transforms.MelSpectrogram(n_fft=n_fft, win_length=win_length,\n",
        "                                                      hop_length=hop_length, pad=pad,\n",
        "                                                      window_fn=th.hann_window,\n",
        "                                                      power=power, normalized=normalized, wkwargs=None)\n",
        "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "        # Initialize the filterbank. Equally spaced in MIDI scale.\n",
        "        harmonic_hz, self.level = initialize_filterbank(\n",
        "            sample_rate, n_harmonic, semitone_scale)\n",
        "\n",
        "        # Center frequncies to tensor\n",
        "        self.f0 = th.tensor(harmonic_hz.astype('float32'))\n",
        "\n",
        "        # Bandwidth parameters\n",
        "        if learn_bw == 'only_Q':\n",
        "            self.bw_Q = nn.Parameter(th.tensor(\n",
        "                np.array([bw_Q]).astype('float32')))\n",
        "        elif learn_bw == 'fix':\n",
        "            self.bw_Q = th.tensor(np.array([bw_Q]).astype('float32'))\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            state_dict = th.load(checkpoint)\n",
        "            hstft_state_dict = {k.replace('hstft.', ''): v for k,\n",
        "                                v in state_dict.items() if 'hstft.' in k}\n",
        "            self.load_state_dict(hstft_state_dict)\n",
        "\n",
        "    def get_harmonic_fb(self):\n",
        "        # bandwidth\n",
        "        bw = (self.bw_alpha * self.f0 + self.bw_beta) / self.bw_Q\n",
        "        bw = bw.unsqueeze(0)  # (1, n_band)\n",
        "        f0 = self.f0.unsqueeze(0)  # (1, n_band)\n",
        "        fft_bins = self.fft_bins.unsqueeze(1)  # (n_bins, 1)\n",
        "\n",
        "        up_slope = th.matmul(fft_bins, (2/bw)) + 1 - (2 * f0 / bw)\n",
        "        down_slope = th.matmul(fft_bins, (-2/bw)) + 1 + (2 * f0 / bw)\n",
        "        fb = th.max(self.zero, th.min(down_slope, up_slope))\n",
        "        return fb\n",
        "\n",
        "    def to_device(self, device, n_bins):\n",
        "        self.f0 = self.f0.to(device)\n",
        "        self.bw_Q = self.bw_Q.to(device)\n",
        "        # fft bins\n",
        "        self.fft_bins = th.linspace(0, self.sample_rate//2, n_bins)\n",
        "        self.fft_bins = self.fft_bins.to(device)\n",
        "        self.zero = th.zeros(1)\n",
        "        self.zero = self.zero.to(device)\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        # stft\n",
        "        spectrogram = self.spec(waveform)\n",
        "        # to device\n",
        "        self.to_device(waveform.device, spectrogram.size(1))\n",
        "        # triangle filter\n",
        "        harmonic_fb = self.get_harmonic_fb()\n",
        "        harmonic_spec = th.matmul(\n",
        "            spectrogram.transpose(1, 2), harmonic_fb).transpose(1, 2)\n",
        "        # (batch, channel, length) -> (batch, harmonic, f0, length)\n",
        "        b, c, l = harmonic_spec.size()\n",
        "        harmonic_spec = harmonic_spec.view(b, self.n_harmonic, self.level, l)\n",
        "        # amplitude to db\n",
        "        harmonic_spec = self.amplitude_to_db(harmonic_spec)\n",
        "        return harmonic_spec"
      ],
      "metadata": {
        "id": "G0U2AOnIbNfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NETWORKS BLOCK##\n",
        "\n",
        "class Res2DMaxPoolModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pooling=2):\n",
        "        super(Res2DMaxPoolModule, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.bn_1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv_2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn_2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.mp = nn.MaxPool2d(tuple(pooling))\n",
        "\n",
        "        # residual\n",
        "        self.diff = False\n",
        "        if in_channels != out_channels:\n",
        "            self.conv_3 = nn.Conv2d(\n",
        "                in_channels, out_channels, 3, padding=1)\n",
        "            self.bn_3 = nn.BatchNorm2d(out_channels)\n",
        "            self.diff = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn_2(self.conv_2(self.relu(self.bn_1(self.conv_1(x)))))\n",
        "        if self.diff:\n",
        "            x = self.bn_3(self.conv_3(x))\n",
        "        out = x + out\n",
        "        out = self.mp(self.relu(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResFrontEnd(nn.Module):\n",
        "    \"\"\"\n",
        "    Adapted from Minz Won ResNet implementation.\n",
        "\n",
        "    Original code: https://github.com/minzwon/semi-supervised-music-tagging-transformer/blob/master/src/modules.py\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, freq_pooling, time_pooling):\n",
        "        super(ResFrontEnd, self).__init__()\n",
        "        self.input_bn = nn.BatchNorm2d(in_channels)\n",
        "        self.layer1 = Res2DMaxPoolModule(\n",
        "            in_channels, out_channels, pooling=(freq_pooling[0], time_pooling[0]))\n",
        "        self.layer2 = Res2DMaxPoolModule(\n",
        "            out_channels, out_channels, pooling=(freq_pooling[1], time_pooling[1]))\n",
        "        self.layer3 = Res2DMaxPoolModule(\n",
        "            out_channels, out_channels, pooling=(freq_pooling[2], time_pooling[2]))\n",
        "\n",
        "    def forward(self, hcqt):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            hcqt: [B, F, K, T]\n",
        "\n",
        "        Outputs:\n",
        "            out: [B, ^F, ^K, ^T]\n",
        "        \"\"\"\n",
        "        # batch normalization\n",
        "        out = self.input_bn(hcqt)\n",
        "\n",
        "        # CNN\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SpecTNTBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_channels, n_frequencies, n_times,\n",
        "        spectral_dmodel, spectral_nheads, spectral_dimff,\n",
        "        temporal_dmodel, temporal_nheads, temporal_dimff,\n",
        "        embed_dim, dropout, use_tct\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.D = embed_dim\n",
        "        self.F = n_frequencies\n",
        "        self.K = n_channels\n",
        "        self.T = n_times\n",
        "\n",
        "        # TCT: Temporal Class Token\n",
        "        if use_tct:\n",
        "            self.T += 1\n",
        "\n",
        "        # Shared frequency-time linear layers\n",
        "        self.D_to_K = nn.Linear(self.D, self.K)\n",
        "        self.K_to_D = nn.Linear(self.K, self.D)\n",
        "\n",
        "        # Spectral Transformer Encoder\n",
        "        self.spectral_linear_in = nn.Linear(self.F+1, spectral_dmodel)\n",
        "        self.spectral_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=spectral_dmodel, nhead=spectral_nheads, dim_feedforward=spectral_dimff, dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
        "        self.spectral_linear_out = nn.Linear(spectral_dmodel, self.F+1)\n",
        "\n",
        "        # Temporal Transformer Encoder\n",
        "        self.temporal_linear_in = nn.Linear(self.T, temporal_dmodel)\n",
        "        self.temporal_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=temporal_dmodel, nhead=temporal_nheads, dim_feedforward=temporal_dimff, dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
        "        self.temporal_linear_out = nn.Linear(temporal_dmodel, self.T)\n",
        "\n",
        "    def forward(self, spec_in, temp_in):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            spec_in: spectral embedding input [B, T, F+1, K]\n",
        "            temp_in: temporal embedding input [B, T, 1, D]\n",
        "\n",
        "        Outputs:\n",
        "            spec_out: spectral embedding output [B, T, F+1, K]\n",
        "            temp_out: temporal embedding output [B, T, 1, D]\n",
        "        \"\"\"\n",
        "        # Element-wise addition between TE and FCT\n",
        "        spec_in = spec_in +             nn.functional.pad(self.D_to_K(temp_in), (0, 0, 0, self.F))\n",
        "\n",
        "        # Spectral Transformer\n",
        "        spec_in = spec_in.flatten(0, 1).transpose(1, 2)  # [B*T, K, F+1]\n",
        "        emb = self.spectral_linear_in(spec_in)  # [B*T, K, spectral_dmodel]\n",
        "        spec_enc_out = self.spectral_encoder_layer(\n",
        "            emb)  # [B*T, K, spectral_dmodel]\n",
        "        spec_out = self.spectral_linear_out(spec_enc_out)  # [B*T, K, F+1]\n",
        "        spec_out = spec_out.view(-1, self.T, self.K,\n",
        "                                 self.F+1).transpose(2, 3)  # [B, T, F+1, K]\n",
        "\n",
        "        # FCT slicing (first raw) + back to D\n",
        "        temp_in = temp_in + self.K_to_D(spec_out[:, :, :1, :])  # [B, T, 1, D]\n",
        "\n",
        "        # Temporal Transformer\n",
        "        temp_in = temp_in.permute(0, 2, 3, 1).flatten(0, 1)  # [B, D, T]\n",
        "        emb = self.temporal_linear_in(temp_in)  # [B, D, temporal_dmodel]\n",
        "        temp_enc_out = self.temporal_encoder_layer(\n",
        "            emb)  # [B, D, temporal_dmodel]\n",
        "        temp_out = self.temporal_linear_out(temp_enc_out)  # [B, D, T]\n",
        "        temp_out = temp_out.unsqueeze(1).permute(0, 3, 1, 2)  # [B, T, 1, D]\n",
        "\n",
        "        return spec_out, temp_out\n",
        "\n",
        "\n",
        "class SpecTNTModule(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_channels, n_frequencies, n_times,\n",
        "        spectral_dmodel, spectral_nheads, spectral_dimff,\n",
        "        temporal_dmodel, temporal_nheads, temporal_dimff,\n",
        "        embed_dim, n_blocks, dropout, use_tct\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        D = embed_dim\n",
        "        F = n_frequencies\n",
        "        K = n_channels\n",
        "        T = n_times\n",
        "\n",
        "        # Frequency Class Token\n",
        "        self.fct = nn.Parameter(th.zeros(1, T, 1, K))\n",
        "\n",
        "        # Frequency Positional Encoding\n",
        "        self.fpe = nn.Parameter(th.zeros(1, 1, F+1, K))\n",
        "\n",
        "        # TCT: Temporal Class Token\n",
        "        if use_tct:\n",
        "            self.tct = nn.Parameter(th.zeros(1, 1, 1, D))\n",
        "        else:\n",
        "            self.tct = None\n",
        "\n",
        "        # Temporal Embedding\n",
        "        self.te = nn.Parameter(th.rand(1, T, 1, D))\n",
        "\n",
        "        # SpecTNT blocks\n",
        "        self.spectnt_blocks = nn.ModuleList([\n",
        "            SpecTNTBlock(\n",
        "                n_channels,\n",
        "                n_frequencies,\n",
        "                n_times,\n",
        "                spectral_dmodel,\n",
        "                spectral_nheads,\n",
        "                spectral_dimff,\n",
        "                temporal_dmodel,\n",
        "                temporal_nheads,\n",
        "                temporal_dimff,\n",
        "                embed_dim,\n",
        "                dropout,\n",
        "                use_tct\n",
        "            )\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x: [B, T, F, K]\n",
        "\n",
        "        Output:\n",
        "            spec_emb: [B, T, F+1, K]\n",
        "            temp_emb: [B, T, 1, D]\n",
        "        \"\"\"\n",
        "        batch_size = len(x)\n",
        "\n",
        "        # Initialize spectral embedding - concat FCT (first raw) + add FPE\n",
        "        fct = th.repeat_interleave(self.fct, batch_size, 0)  # [B, T, 1, K]\n",
        "        spec_emb = th.cat([fct, x], dim=2)  # [B, T, F+1, K]\n",
        "        spec_emb = spec_emb + self.fpe\n",
        "        if self.tct is not None:\n",
        "            spec_emb = nn.functional.pad(\n",
        "                spec_emb, (0, 0, 0, 0, 1, 0))  # [B, T+1, F+1, K]\n",
        "\n",
        "        # Initialize temporal embedding\n",
        "        temp_emb = th.repeat_interleave(self.te, batch_size, 0)  # [B, T, 1, D]\n",
        "        if self.tct is not None:\n",
        "            tct = th.repeat_interleave(self.tct, batch_size, 0)  # [B, 1, 1, D]\n",
        "            temp_emb = th.cat([tct, temp_emb], dim=1)  # [B, T+1, 1, D]\n",
        "\n",
        "        # SpecTNT blocks inference\n",
        "        for block in self.spectnt_blocks:\n",
        "            spec_emb, temp_emb = block(spec_emb, temp_emb)\n",
        "\n",
        "        return spec_emb, temp_emb\n",
        "\n",
        "\n",
        "class SpecTNT(nn.Module):\n",
        "    def __init__(\n",
        "        self, fe_model,\n",
        "        n_channels, n_frequencies, n_times,\n",
        "        spectral_dmodel, spectral_nheads, spectral_dimff,\n",
        "        temporal_dmodel, temporal_nheads, temporal_dimff,\n",
        "        embed_dim, n_blocks, dropout, use_tct, n_classes\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TCT: Temporal Class Token\n",
        "        self.use_tct = use_tct\n",
        "\n",
        "        # Front-end model\n",
        "        self.fe_model = fe_model\n",
        "\n",
        "        # Main model\n",
        "        self.main_model = SpecTNTModule(\n",
        "            n_channels,\n",
        "            n_frequencies,\n",
        "            n_times,\n",
        "            spectral_dmodel,\n",
        "            spectral_nheads,\n",
        "            spectral_dimff,\n",
        "            temporal_dmodel,\n",
        "            temporal_nheads,\n",
        "            temporal_dimff,\n",
        "            embed_dim,\n",
        "            n_blocks,\n",
        "            dropout,\n",
        "            use_tct\n",
        "        )\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear_out = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            features: [B, K, F, T]\n",
        "\n",
        "        Output:\n",
        "            logits:\n",
        "                - [B, n_classes] if use_tct\n",
        "                - [B, T, n_classes] otherwise\n",
        "        \"\"\"\n",
        "        # Add channel dimension if None\n",
        "        if len(features.size()) == 3:\n",
        "            features = features.unsqueeze(1)\n",
        "        # Front-end model\n",
        "        fe_out = self.fe_model(features)            # [B, ^K, ^F, ^T]\n",
        "        fe_out = fe_out.permute(0, 3, 2, 1)         # [B, T, F, K]\n",
        "        # Main model\n",
        "        _, temp_emb = self.main_model(fe_out)       # [B, T, 1, D]\n",
        "        # Linear layer\n",
        "        if self.use_tct:\n",
        "            return self.linear_out(temp_emb[:, 0, 0, :])   # [B, n_classes]\n",
        "        else:\n",
        "            return self.linear_out(temp_emb[:, :, 0, :])   # [B, T, n_classes]"
      ],
      "metadata": {
        "id": "zmEqWa7EbX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(pl.LightningModule):\n",
        "    def __init__(self, feature_extractor, net, optimizer, lr_scheduler, criterion, datamodule, activation_fn):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.net = net\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.criterion = criterion\n",
        "        self.datamodule = datamodule\n",
        "\n",
        "        if activation_fn == \"softmax\":\n",
        "            self.activation = nn.Softmax(dim=2)\n",
        "        elif activation_fn == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.lr_scheduler is None:\n",
        "            return {\"optimizer\": self.optimizer}\n",
        "        else:\n",
        "            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.lr_scheduler, \"monitor\": \"val_loss\"}\n",
        "\n",
        "    @staticmethod\n",
        "    def _classname(obj, lower=True):\n",
        "        if hasattr(obj, '__name__'):\n",
        "            name = obj.__name__\n",
        "        else:\n",
        "            name = obj.__class__.__name__\n",
        "        return name.lower() if lower else name\n",
        "\n",
        "\n",
        "class BeatEstimator(BaseModel):\n",
        "    def __init__(self, feature_extractor, net, optimizer, lr_scheduler, criterion, datamodule, activation_fn):\n",
        "        super().__init__(\n",
        "            feature_extractor,\n",
        "            net,\n",
        "            optimizer,\n",
        "            lr_scheduler,\n",
        "            criterion,\n",
        "            datamodule,\n",
        "            activation_fn\n",
        "        )\n",
        "\n",
        "        self.target_fps = datamodule.sample_rate /             (datamodule.hop_length * datamodule.pooling_shrinking)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        losses = {}\n",
        "        x, y = batch['audio'], batch['targets']\n",
        "        features = self.feature_extractor(x)\n",
        "        logits = self.net(features)\n",
        "        losses['train_loss'] = self.criterion(\n",
        "            logits.flatten(end_dim=1), y.flatten(end_dim=1))\n",
        "        self.log_dict(losses, on_step=False, on_epoch=True)\n",
        "        return losses['train_loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        losses = {}\n",
        "        audio, targets, ref_beats, ref_downbeats = (\n",
        "            batch['audio'][0],\n",
        "            batch['targets'][0].cpu(),\n",
        "            batch['beats'][0].cpu(),\n",
        "            batch['downbeats'][0].cpu()\n",
        "        )\n",
        "        input_length, sample_rate, batch_size = (\n",
        "            self.datamodule.input_length,\n",
        "            self.datamodule.sample_rate,\n",
        "            self.datamodule.batch_size\n",
        "        )\n",
        "        audio_chunks = th.cat([el.unsqueeze(0) for el in audio.split(\n",
        "            split_size=int(input_length*sample_rate))[:-1]], dim=0)\n",
        "        # Inference loop\n",
        "        logits_list, probs_list = th.tensor([]), th.tensor([])\n",
        "        for batch_audio in audio_chunks.split(batch_size):\n",
        "            with th.no_grad():\n",
        "                features = self.feature_extractor(batch_audio)\n",
        "                logits = self.net(features)\n",
        "                probs = self.activation(logits)\n",
        "                logits_list = th.cat(\n",
        "                    [logits_list, logits.flatten(end_dim=1).cpu()], dim=0)\n",
        "                probs_list = th.cat(\n",
        "                    [probs_list, probs.flatten(end_dim=1).cpu()], dim=0)\n",
        "        # Postprocessing\n",
        "        beats_data = probs_list.argmax(dim=1)\n",
        "        est_beats = th.where(beats_data == 0)[0] / self.target_fps\n",
        "        est_downbeats = th.where(beats_data == 1)[0] / self.target_fps\n",
        "        # Eval\n",
        "        losses['val_loss'] = self.criterion(\n",
        "            logits_list, targets[:len(logits_list)])\n",
        "        losses['beats_f_measure'] = mir_eval.beat.f_measure(\n",
        "            ref_beats, est_beats)\n",
        "        losses['downbeats_f_measure'] = mir_eval.beat.f_measure(\n",
        "            ref_downbeats, est_downbeats)\n",
        "        self.log_dict(losses, on_step=False, on_epoch=True)\n",
        "        return losses['val_loss']"
      ],
      "metadata": {
        "id": "m4Pi3A6nbkdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##DUMMY DATASETS BLOCK##\n",
        "\n",
        "class DummyBeatDataset(tud.Dataset):\n",
        "\n",
        "    def __init__(self, sample_rate, input_length, hop_length, time_shrinking, mode):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_length = input_length\n",
        "\n",
        "        self.target_fps = sample_rate / (hop_length * time_shrinking)\n",
        "        self.target_nframes = int(input_length * self.target_fps)\n",
        "\n",
        "        assert mode in [\"train\", \"validation\", \"test\"]\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return 80\n",
        "        elif self.mode == \"validation\":\n",
        "            return 10\n",
        "        elif self.mode == \"test\":\n",
        "            return 10\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.mode == \"train\":\n",
        "            return {\n",
        "                'audio': th.zeros(self.input_length * self.sample_rate),\n",
        "                'targets': th.zeros(self.target_nframes, 3)\n",
        "            }\n",
        "        elif self.mode in [\"validation\", \"test\"]:\n",
        "            return {\n",
        "                'audio': th.zeros(10 * self.input_length * self.sample_rate),\n",
        "                'targets': th.zeros(10 * self.target_nframes, 3),\n",
        "                'beats': th.arange(0, 50, 0.5),\n",
        "                'downbeats': th.arange(0, 50, 2.)\n",
        "            }\n",
        "\n",
        "\n",
        "class DummyBeatDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, n_workers, pin_memory, sample_rate, input_length, hop_length, time_shrinking):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_workers = n_workers\n",
        "        self.pin_memory = pin_memory\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_length = input_length\n",
        "        self.hop_length = hop_length\n",
        "        self.time_shrinking = time_shrinking\n",
        "        self._log_hyperparams = False\n",
        "        self.allow_zero_length_dataloader_with_multiple_devices = True\n",
        "\n",
        "\n",
        "    def setup(self, stage):\n",
        "        self.train_set = DummyBeatDataset(\n",
        "            self.sample_rate,\n",
        "            self.input_length,\n",
        "            self.hop_length,\n",
        "            self.time_shrinking,\n",
        "            \"train\"\n",
        "        )\n",
        "        self.val_set = DummyBeatDataset(\n",
        "            self.sample_rate,\n",
        "            self.input_length,\n",
        "            self.hop_length,\n",
        "            self.time_shrinking,\n",
        "            \"validation\"\n",
        "        )\n",
        "        self.test_set = DummyBeatDataset(\n",
        "            self.sample_rate,\n",
        "            self.input_length,\n",
        "            self.hop_length,\n",
        "            self.time_shrinking,\n",
        "            \"test\"\n",
        "        )\n",
        "\n",
        "    def prepare_data_per_node(self):\n",
        "        return None\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return tud.DataLoader(self.train_set,\n",
        "                              batch_size=self.batch_size,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=True,\n",
        "                              num_workers=self.n_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return tud.DataLoader(self.val_set,\n",
        "                              batch_size=1,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=False,\n",
        "                              num_workers=self.n_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return tud.DataLoader(self.test_set,\n",
        "                              batch_size=1,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=False,\n",
        "                              num_workers=self.n_workers)"
      ],
      "metadata": {
        "id": "Y83iObfSbfPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BALLROOM DATASETS ##\n",
        "\n",
        "class BallroomDataset(tud.Dataset):\n",
        "    def __init__(self, audio_dir, annotation_dir, sample_rate, input_length, hop_length, fft_win_length, pooling_shrinking, mode):\n",
        "        super(BallroomDataset, self).__init__()\n",
        "\n",
        "        assert mode in [\"train\", \"validation\", \"test\"]\n",
        "        self.mode = mode\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_length = input_length\n",
        "        self.hop_length = hop_length\n",
        "        self.fft_win_length = fft_win_length\n",
        "        self.pooling_shrinking = pooling_shrinking\n",
        "\n",
        "        audio_files_index = \"\"\n",
        "        if self.mode == \"train\":\n",
        "            audio_files_index = os.path.join(audio_dir, \"train.txt\")\n",
        "        elif self.mode == \"validation\":\n",
        "            audio_files_index = os.path.join(audio_dir, \"val.txt\")\n",
        "        elif self.mode == \"test\":\n",
        "            audio_files_index = os.path.join(audio_dir, \"test.txt\")\n",
        "\n",
        "        # get audio files for given mode\n",
        "        self.audio_files = []\n",
        "        with open(audio_files_index, \"r\") as f:\n",
        "            for audio_file in f:\n",
        "                audio_file = audio_file.strip()\n",
        "                self.audio_files.append(os.path.join(audio_dir, audio_file))\n",
        "\n",
        "        # get annotation files for given mode\n",
        "        self.ann_files = []\n",
        "        for audio_file in self.audio_files:\n",
        "            self.ann_files.append(os.path.join(annotation_dir, os.path.splitext(os.path.basename(audio_file))[0] + \".beats\"))\n",
        "\n",
        "        # get audio chunks for given mode\n",
        "        self.audio_chunks = [] # ('index into self.audio_files', 'offset in seconds from start of audio file')\n",
        "        if self.mode == \"train\":\n",
        "            for i, audio_file in enumerate(self.audio_files):\n",
        "                sample_rate = librosa.get_samplerate(audio_file)\n",
        "                duration = librosa.get_duration(path=audio_file, sr=sample_rate)\n",
        "                offsets = th.arange(0, int(duration - self.input_length) + 1)\n",
        "                for offset in offsets:\n",
        "                    self.audio_chunks.append((i, offset))\n",
        "            shuffle(self.audio_chunks)\n",
        "        else:\n",
        "            for i, _ in enumerate(self.audio_files):\n",
        "                self.audio_chunks.append((i, 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_chunks)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx, chunk_offset_in_seconds = self.audio_chunks[i]\n",
        "\n",
        "        # load audio data\n",
        "        audio_file = self.audio_files[idx]\n",
        "        waveform, sample_rate = torchaudio.load(audio_file)\n",
        "        nsamples = tuple(waveform.shape)[1]\n",
        "        waveform = waveform.flatten()\n",
        "        #duration = nsamples / sample_rate\n",
        "\n",
        "        # load annotation data\n",
        "        ann_file = self.ann_files[idx]\n",
        "        beat_offsets_in_seconds = []\n",
        "        beats = []\n",
        "        with open(ann_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = line.split(\" \")\n",
        "                beat_offsets_in_seconds.append(float(line[0]))\n",
        "                beats.append(int(line[1]))\n",
        "\n",
        "        # generate targets\n",
        "        nframes = int((1 + (nsamples - self.fft_win_length) // self.hop_length) // self.pooling_shrinking)\n",
        "        samples_per_frame = nsamples / nframes\n",
        "\n",
        "        targets = [[0,0,1] for i in range(nframes)] # init to 'non-beat'\n",
        "        for i, beat_offset_in_seconds in enumerate(beat_offsets_in_seconds):\n",
        "            beat_offset_in_samples = beat_offset_in_seconds * self.sample_rate\n",
        "            beat_offset_in_frames = int(beat_offset_in_samples // samples_per_frame)\n",
        "            if beat_offset_in_frames < len(targets):\n",
        "                if beats[i] == 1: # downbeat\n",
        "                    targets[beat_offset_in_frames] = [0,1,0] # set to 'downbeat'\n",
        "                else: # beat\n",
        "                    targets[beat_offset_in_frames] = [1,0,0] # set to 'beat'\n",
        "\n",
        "        targets = th.Tensor(targets)\n",
        "\n",
        "        # return mode-specific data\n",
        "        if self.mode == \"train\":\n",
        "            # compute audio chunk\n",
        "            chunk_length_in_samples = self.input_length * self.sample_rate\n",
        "            chunk_offset_in_samples = chunk_offset_in_seconds * self.sample_rate\n",
        "            audio_chunk = waveform[chunk_offset_in_samples:chunk_offset_in_samples + chunk_length_in_samples]\n",
        "\n",
        "            # compute targets chunk\n",
        "            chunk_length_in_frames = int((1 + (chunk_length_in_samples - self.fft_win_length) // self.hop_length) // self.pooling_shrinking)\n",
        "            chunk_offset_in_frames = int(chunk_offset_in_samples // samples_per_frame)\n",
        "            targets_chunk = targets[chunk_offset_in_frames:chunk_offset_in_frames + chunk_length_in_frames, :]\n",
        "\n",
        "            audio, target = self.apply_augmentations(audio_chunk, targets_chunk)\n",
        "\n",
        "            return {\n",
        "                'audio': audio,\n",
        "                'targets': target\n",
        "            }\n",
        "        elif self.mode in [\"validation\", \"test\"]:\n",
        "            # get downbeat offsets\n",
        "            downbeat_offsets_in_seconds = []\n",
        "            for i, beat in enumerate(beats):\n",
        "                if beat == 1:\n",
        "                    downbeat_offsets_in_seconds.append(beat_offsets_in_seconds[i])\n",
        "\n",
        "            return {\n",
        "                'audio': waveform,\n",
        "                'targets': targets,\n",
        "                'beats': th.Tensor(beat_offsets_in_seconds),\n",
        "                'downbeats': th.Tensor(downbeat_offsets_in_seconds)\n",
        "            }\n",
        "\n",
        "    def apply_augmentations(self, audio, target):\n",
        "\n",
        "        # random gain from 0dB to -6 dB\n",
        "        #if np.random.rand() < 0.2:\n",
        "        #    #sgn = np.random.choice([-1,1])\n",
        "        #    audio = audio * (10**((-1 * np.random.rand() * 6)/20))\n",
        "\n",
        "        # phase inversion\n",
        "        if np.random.rand() < 0.5:\n",
        "            audio = -audio\n",
        "\n",
        "        # drop continguous frames\n",
        "        if np.random.rand() < 0.05:\n",
        "            zero_size = int(self.length*0.1)\n",
        "            start = np.random.randint(audio.shape[-1] - zero_size - 1)\n",
        "            stop = start + zero_size\n",
        "            audio[:,start:stop] = 0\n",
        "            target[:,start:stop] = 0\n",
        "\n",
        "        # apply time stretching\n",
        "        # if np.random.rand() < 0.0:\n",
        "        #     factor = np.random.normal(1.0, 0.5)\n",
        "        #     factor = np.clip(factor, a_min=0.6, a_max=1.8)\n",
        "\n",
        "        #     tfm = sox.Transformer()\n",
        "\n",
        "        #     if abs(factor - 1.0) <= 0.1: # use stretch\n",
        "        #         tfm.stretch(1/factor)\n",
        "        #     else:   # use tempo\n",
        "        #         tfm.tempo(factor, 'm')\n",
        "\n",
        "        #     audio = tfm.build_array(input_array=audio.squeeze().numpy(),\n",
        "        #                             sample_rate_in=self.audio_sample_rate)\n",
        "        #     audio = th.from_numpy(audio.astype('float32')).view(1,-1)\n",
        "\n",
        "        #     # now we update the targets based on new tempo\n",
        "        #     dbeat_ind = (target[1,:] == 1).nonzero(as_tuple=False)\n",
        "        #     dbeat_sec = dbeat_ind / self.target_sample_rate\n",
        "        #     new_dbeat_sec = (dbeat_sec / factor).squeeze()\n",
        "        #     new_dbeat_ind = (new_dbeat_sec * self.target_sample_rate).long()\n",
        "\n",
        "        #     beat_ind = (target[0,:] == 1).nonzero(as_tuple=False)\n",
        "        #     beat_sec = beat_ind / self.target_sample_rate\n",
        "        #     new_beat_sec = (beat_sec / factor).squeeze()\n",
        "        #     new_beat_ind = (new_beat_sec * self.target_sample_rate).long()\n",
        "\n",
        "        #     # now convert indices back to target vector\n",
        "        #     new_size = int(np.ceil(target.shape[-1] / factor))\n",
        "        #     streteched_target = th.zeros(2,new_size)\n",
        "        #     streteched_target[0,new_beat_ind] = 1\n",
        "        #     streteched_target[1,new_dbeat_ind] = 1\n",
        "        #     target = streteched_target\n",
        "\n",
        "        # shift targets forward/back max 70ms\n",
        "        if np.random.rand() < 0.3:\n",
        "\n",
        "            # in this method we shift each beat and downbeat by a random amount\n",
        "            max_shift = int(0.045 * self.target_sample_rate)\n",
        "\n",
        "            beat_ind = th.logical_and(target[0,:] == 1, target[1,:] != 1).nonzero(as_tuple=False) # all beats EXCEPT downbeats\n",
        "            dbeat_ind = (target[1,:] == 1).nonzero(as_tuple=False)\n",
        "\n",
        "            # shift just the downbeats\n",
        "            dbeat_shifts = th.normal(0.0, max_shift/2, size=(1,dbeat_ind.shape[-1]))\n",
        "            dbeat_ind += dbeat_shifts.long()\n",
        "\n",
        "            # now shift the non-downbeats\n",
        "            beat_shifts = th.normal(0.0, max_shift/2, size=(1,beat_ind.shape[-1]))\n",
        "            beat_ind += beat_shifts.long()\n",
        "\n",
        "            # ensure we have no beats beyond max index\n",
        "            beat_ind = beat_ind[beat_ind < target.shape[-1]]\n",
        "            dbeat_ind = dbeat_ind[dbeat_ind < target.shape[-1]]\n",
        "\n",
        "            # now convert indices back to target vector\n",
        "            shifted_target = th.zeros(2,target.shape[-1])\n",
        "            shifted_target[0,beat_ind] = 1\n",
        "            shifted_target[0,dbeat_ind] = 1 # set also downbeats on first channel\n",
        "            shifted_target[1,dbeat_ind] = 1\n",
        "\n",
        "            target = shifted_target\n",
        "\n",
        "        # apply pitch shifting\n",
        "        # if np.random.rand() < 0.5:\n",
        "        #     sgn = np.random.choice([-1,1])\n",
        "        #     factor = sgn * np.random.rand() * 8.0\n",
        "        #     tfm = sox.Transformer()\n",
        "        #     tfm.pitch(factor)\n",
        "        #     audio = tfm.build_array(input_array=audio.squeeze().numpy(),\n",
        "        #                             sample_rate_in=self.audio_sample_rate)\n",
        "        #     audio = th.from_numpy(audio.astype('float32')).view(1,-1)\n",
        "\n",
        "        # apply a lowpass filter\n",
        "        if np.random.rand() < 0.1:\n",
        "            cutoff = (np.random.rand() * 4000) + 4000\n",
        "            sos = scipy.signal.butter(2,\n",
        "                                      cutoff,\n",
        "                                      btype=\"lowpass\",\n",
        "                                      fs=self.audio_sample_rate,\n",
        "                                      output='sos')\n",
        "            audio_filtered = scipy.signal.sosfilt(sos, audio.numpy())\n",
        "            audio = th.from_numpy(audio_filtered.astype('float32'))\n",
        "\n",
        "        # apply a highpass filter\n",
        "        if np.random.rand() < 0.1:\n",
        "            cutoff = (np.random.rand() * 1000) + 20\n",
        "            sos = scipy.signal.butter(2,\n",
        "                                      cutoff,\n",
        "                                      btype=\"highpass\",\n",
        "                                      fs=self.audio_sample_rate,\n",
        "                                      output='sos')\n",
        "            audio_filtered = scipy.signal.sosfilt(sos, audio.numpy())\n",
        "            audio = th.from_numpy(audio_filtered.astype('float32'))\n",
        "\n",
        "        # apply a chorus effect\n",
        "        # if np.random.rand() < 0.05:\n",
        "        #     tfm = sox.Transformer()\n",
        "        #     tfm.chorus()\n",
        "        #     audio = tfm.build_array(input_array=audio.squeeze().numpy(),\n",
        "        #                             sample_rate_in=self.audio_sample_rate)\n",
        "        #     audio = th.from_numpy(audio.astype('float32')).view(1,-1)\n",
        "\n",
        "        # apply a compressor effect\n",
        "        # if np.random.rand() < 0.15:\n",
        "        #     attack = (np.random.rand() * 0.300) + 0.005\n",
        "        #     release = (np.random.rand() * 1.000) + 0.3\n",
        "        #     tfm = sox.Transformer()\n",
        "        #     tfm.compand(attack_time=attack, decay_time=release)\n",
        "        #     audio = tfm.build_array(input_array=audio.squeeze().numpy(),\n",
        "        #                             sample_rate_in=self.audio_sample_rate)\n",
        "        #     audio = th.from_numpy(audio.astype('float32')).view(1,-1)\n",
        "\n",
        "        # apply an EQ effect\n",
        "        if np.random.rand() < 0.15:\n",
        "            freq = (np.random.rand() * 8000) + 60\n",
        "            q = (np.random.rand() * 7.0) + 0.1\n",
        "            g = np.random.normal(0.0, 6)\n",
        "            tfm = sox.Transformer()\n",
        "            tfm.equalizer(frequency=freq, width_q=q, gain_db=g)\n",
        "            audio = tfm.build_array(input_array=audio.squeeze().numpy(),\n",
        "                                    sample_rate_in=self.audio_sample_rate)\n",
        "            audio = th.from_numpy(audio.astype('float32')).view(1,-1)\n",
        "\n",
        "        # add white noise\n",
        "        if np.random.rand() < 0.05:\n",
        "            wn = (th.rand(audio.shape) * 2) - 1\n",
        "            g = 10**(-(np.random.rand() * 20) - 12)/20\n",
        "            audio = audio + (g * wn)\n",
        "\n",
        "        # apply nonlinear distortion\n",
        "        if np.random.rand() < 0.2:\n",
        "            g = 10**((np.random.rand() * 12)/20)\n",
        "            audio = th.tanh(audio)\n",
        "\n",
        "        # normalize the audio\n",
        "        audio /= audio.float().abs().max()\n",
        "\n",
        "        return audio, target\n",
        "\n",
        "class BallroomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, n_workers, pin_memory, audio_dir, annotation_dir, sample_rate, input_length, hop_length, fft_win_length, pooling_shrinking):\n",
        "        self.batch_size = batch_size\n",
        "        self.n_workers = n_workers\n",
        "        self.pin_memory = pin_memory\n",
        "        self.audio_dir = audio_dir\n",
        "        self.annotation_dir = annotation_dir\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_length = input_length\n",
        "        self.hop_length = hop_length\n",
        "        self.fft_win_length = fft_win_length\n",
        "        self.pooling_shrinking = pooling_shrinking\n",
        "        self._log_hyperparams = False\n",
        "        self.allow_zero_length_dataloader_with_multiple_devices = True\n",
        "\n",
        "    def setup(self, stage):\n",
        "        self.train_set = BallroomDataset(\n",
        "            audio_dir=self.audio_dir,\n",
        "            annotation_dir=self.annotation_dir,\n",
        "            sample_rate=self.sample_rate,\n",
        "            input_length=self.input_length,\n",
        "            hop_length=self.hop_length,\n",
        "            fft_win_length=self.fft_win_length,\n",
        "            pooling_shrinking=self.pooling_shrinking,\n",
        "            mode=\"train\"\n",
        "        )\n",
        "        self.val_set = BallroomDataset(\n",
        "            audio_dir=self.audio_dir,\n",
        "            annotation_dir=self.annotation_dir,\n",
        "            sample_rate=self.sample_rate,\n",
        "            input_length=self.input_length,\n",
        "            hop_length=self.hop_length,\n",
        "            fft_win_length=self.fft_win_length,\n",
        "            pooling_shrinking=self.pooling_shrinking,\n",
        "            mode=\"validation\"\n",
        "        )\n",
        "        self.test_set = BallroomDataset(\n",
        "            audio_dir=self.audio_dir,\n",
        "            annotation_dir=self.annotation_dir,\n",
        "            sample_rate=self.sample_rate,\n",
        "            input_length=self.input_length,\n",
        "            hop_length=self.hop_length,\n",
        "            fft_win_length=self.fft_win_length,\n",
        "            pooling_shrinking=self.pooling_shrinking,\n",
        "            mode=\"test\"\n",
        "        )\n",
        "\n",
        "    def prepare_data_per_node(self):\n",
        "        return None\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return tud.DataLoader(self.train_set,\n",
        "                              batch_size=self.batch_size,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=True,\n",
        "                              num_workers=self.n_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return tud.DataLoader(self.val_set,\n",
        "                              batch_size=1,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=False,\n",
        "                              num_workers=self.n_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return tud.DataLoader(self.test_set,\n",
        "                              batch_size=1,\n",
        "                              pin_memory=self.pin_memory,\n",
        "                              shuffle=False,\n",
        "                              augment=False,num_workers=self.n_workers)"
      ],
      "metadata": {
        "id": "P2bllM0NcZBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "8f5d069cd26a416c9b4f59a32dac40b1",
            "97c8524b78dc4ca2807521fe51161169",
            "cd7c9a5fdb6e445ebc913515e3c68c9a",
            "d45664a1b89d417096675db8ad71b28a",
            "63a9395bc99b4b6fa26c7738cf1b6d33",
            "f4e547ce065046c99e3e400a2e206819",
            "e4792eaa56b14da4a3611af50c92f2c1",
            "5b20f26e1ddf4ccb940e291fce70ed47",
            "90e5922019aa4be7bfdeaa1c10040ec0",
            "9619226a55bc4a2194e534d9a8cc1087",
            "3627e2c1b088498e976ebefec65b0042",
            "84f210de653e4df7a523331b4166ebb3",
            "c56f837cfff1476984190a5a90b8b56d",
            "fac267d82a5541e7a2fb8567ed6fc8cb",
            "90e2ed1492e74fa888fdc89ca5f45911",
            "88d01849542945aa91424183e8c371fb",
            "022235382f0d4095ba1c2557eea5b1cb",
            "34aea251c76d46149789cd73bca5733c",
            "2c8a2e0b31744d81b3e7fe9c08cf0e68",
            "7c758b8801bf4cf18ebecf3488d3486e",
            "51f17ecd27fb45d1863a88a97c1c2b29",
            "30c733a00cc4446aa02acbe5beba72e8"
          ]
        },
        "id": "r5_RglBKjvzT",
        "outputId": "968b7177-ed28-42a7-e179-82d1ad0259b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name              | Type             | Params\n",
            "-------------------------------------------------------\n",
            "0 | feature_extractor | HarmonicSTFT     | 1     \n",
            "1 | net               | SpecTNT          | 5.7 M \n",
            "2 | criterion         | CrossEntropyLoss | 0     \n",
            "3 | activation        | Softmax          | 0     \n",
            "-------------------------------------------------------\n",
            "5.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.7 M     Total params\n",
            "22.656    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f5d069cd26a416c9b4f59a32dac40b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84f210de653e4df7a523331b4166ebb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=3` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(precision=32, accumulate_grad_batches= 16, check_val_every_n_epoch= 2, max_steps= 3)\n",
        "\n",
        "feature_extractor = HarmonicSTFT(sample_rate=16000, n_fft=512, n_harmonic=6, semitone_scale=2, learn_bw = 'only_Q')\n",
        "fe_model = ResFrontEnd(in_channels=6, out_channels=256, freq_pooling=[2,2,2], time_pooling=[2,2,1])\n",
        "net = SpecTNT(fe_model = fe_model, n_channels=256, n_frequencies=16, n_times=215, embed_dim=128, spectral_dmodel=64, spectral_nheads=4, spectral_dimff=64,\n",
        "                           temporal_dmodel=256, temporal_nheads=8, temporal_dimff=256, n_blocks=5, dropout=0.15, use_tct=False, n_classes=3)\n",
        "optimizer = th.optim.AdamW(params=net.parameters())\n",
        "criterion = th.nn.CrossEntropyLoss()\n",
        "#datamodule = DummyBeatDataModule(batch_size=2, n_workers=4, pin_memory=False, sample_rate=16000, input_length=5,\n",
        "#                                                    hop_length=256, time_shrinking=4)\n",
        "datamodule = BallroomDataModule(batch_size=2, n_workers=4, pin_memory=False, audio_dir=\"./data/BallroomData/\",\n",
        "                                annotation_dir=\"./data/BallroomAnnotations/\", sample_rate=44100, input_length=5,\n",
        "                                hop_length=256, fft_win_length=512, pooling_shrinking=4)\n",
        "model = BeatEstimator(feature_extractor=feature_extractor, net=net, optimizer=optimizer,\n",
        "                                       lr_scheduler=None, criterion=criterion, datamodule=datamodule, activation_fn= 'softmax')\n",
        "\n",
        "logger = pl.loggers.tensorboard.TensorBoardLogger(name = \"\", save_dir= \"Logger\")\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSdmEGRSj8-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}